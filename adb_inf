# Databricks RAG pipeline using built-in embeddings + 3-layer Medallion Architecture
# Author: ChatGPT

# =====================================================
# 1️⃣  Imports
# =====================================================
import io
from pyspark.sql import functions as F, types as T
from PyPDF2 import PdfReader
from databricks.vector_search.client import VectorSearchClient
from databricks.sdk import WorkspaceClient

# =====================================================
# 2️⃣  Paths & configuration
# =====================================================
PDF_BASE_PATH = "abfss://eeplatform@sa13701.dfs.core.windows.net/unstrutured_dcoumentes/"

CATALOG = "main"                     # replace if needed
SCHEMA = "rag_demo"
BRONZE_TABLE = f"{CATALOG}.{SCHEMA}.pdf_bronze"
SILVER_TABLE = f"{CATALOG}.{SCHEMA}.pdf_silver"
INDEX_NAME = f"{CATALOG}.{SCHEMA}.pdf_rag_index"
ENDPOINT_NAME = "rag-vector-endpoint"

EMBEDDING_MODEL = "databricks-bge-large-en"
LLM_ENDPOINT = "databricks-dbrx-instruct"

# =====================================================
# 3️⃣  Bronze Layer – Read PDFs from ADLS and extract text
# =====================================================
def extract_text_from_pdf_bytes(b: bytes) -> str:
    try:
        reader = PdfReader(io.BytesIO(b))
        return "\n".join([p.extract_text() or "" for p in reader.pages])
    except Exception:
        return ""

extract_text_udf = F.udf(extract_text_from_pdf_bytes, T.StringType())

pdf_df = (
    spark.read.format("binaryFile")
    .option("pathGlobFilter", "*.pdf")
    .load(PDF_BASE_PATH)
    .withColumn("raw_text", extract_text_udf(F.col("content")))
    .withColumn("ingest_ts", F.current_timestamp())
)

pdf_df.write.mode("overwrite").saveAsTable(BRONZE_TABLE)

# =====================================================
# 4️⃣  Silver Layer – Clean & chunk text
# =====================================================
def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 100) -> list:
    if not text:
        return []
    chunks, start = [], 0
    while start < len(text):
        end = min(len(text), start + chunk_size)
        chunks.append(text[start:end])
        start += chunk_size - overlap
    return chunks

chunk_text_udf = F.udf(chunk_text, T.ArrayType(T.StringType()))

bronze_df = spark.table(BRONZE_TABLE)

silver_df = (
    bronze_df
    .withColumn("chunks", chunk_text_udf(F.col("raw_text")))
    .withColumn("chunk", F.posexplode(F.col("chunks")))
    .select(
        F.monotonically_increasing_id().alias("chunk_id"),
        F.col("path"),
        F.col("chunk.pos").alias("chunk_index"),
        F.col("chunk.col").alias("chunk_text"),
        F.current_timestamp().alias("processed_ts")
    )
)

silver_df.write.mode("overwrite").saveAsTable(SILVER_TABLE)

# =====================================================
# 5️⃣  Gold Layer – Create Vector Search Index with built-in embeddings
# =====================================================
vsc = VectorSearchClient()

try:
    vsc.create_endpoint(name=ENDPOINT_NAME, endpoint_type="STANDARD")
except Exception as e:
    print(f"Endpoint exists or creation skipped: {e}")

try:
    vsc.create_delta_sync_index(
        endpoint_name=ENDPOINT_NAME,
        index_name=INDEX_NAME,
        source_table_name=SILVER_TABLE,
        pipeline_type="TRIGGERED",
        primary_key="chunk_id",
        embedding_model=EMBEDDING_MODEL,
        embedding_source_column="chunk_text"
    )
except Exception as e:
    print(f"Index exists or creation skipped: {e}")

# =====================================================
# 6️⃣  RAG Query Function
# =====================================================
ws = WorkspaceClient()
index = vsc.get_index(endpoint_name=ENDPOINT_NAME, index_name=INDEX_NAME)

def rag_query(question: str, top_k: int = 3):
    results = index.similarity_search(
        query_text=question,
        columns=["chunk_text", "path"],
        num_results=top_k
    )
    rows = results["result"]["data_array"]
    context = "\n".join([r[0] for r in rows])

    prompt = f"""You are a helpful assistant. 
    Answer the question based on the context below.
    Context:
    {context}

    Question: {question}
    Answer:"""

    resp = ws.serving_endpoints.query(
        name=LLM_ENDPOINT,
        inputs=[{"role": "user", "content": prompt}]
    )
    return resp.outputs[0].content[0].text

# =====================================================
# 7️⃣  Example Query
# =====================================================
print(rag_query("Summarize the main points of the business document."))
