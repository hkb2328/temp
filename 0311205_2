import threading
import queue
import json
import time
from datetime import datetime

class AsyncADLSLogger:
    def __init__(self, log_dir, flush_interval=10, batch_size=20):
        """
        log_dir: ADLS path, e.g. 'abfss://logs@<storage>.dfs.core.windows.net/pipeline_logs'
        """
        self.log_dir = log_dir.rstrip("/")
        self.flush_interval = flush_interval
        self.batch_size = batch_size
        self.log_queue = queue.Queue()
        self.stop_event = threading.Event()
        self.thread = threading.Thread(target=self._worker, daemon=True)
        self.thread.start()

    def _worker(self):
        buffer = []
        while not self.stop_event.is_set() or not self.log_queue.empty():
            try:
                record = self.log_queue.get(timeout=self.flush_interval)
                buffer.append(record)
            except queue.Empty:
                pass

            # flush if enough logs collected or when stopping
            if len(buffer) >= self.batch_size or (self.stop_event.is_set() and buffer):
                try:
                    self._flush(buffer)
                except Exception as e:
                    print(f"[LOGGER] Flush failed: {e}")
                buffer = []

    def _flush(self, buffer):
        if not buffer:
            return

        from pyspark.dbutils import DBUtils
        from pyspark.sql import SparkSession

        spark = SparkSession.getActiveSession()
        dbutils = DBUtils(spark)

        date_str = datetime.utcnow().strftime("%Y%m%d")
        timestamp_str = datetime.utcnow().strftime("%H%M%S")
        log_file = f"{self.log_dir}/logs_{date_str}_{timestamp_str}.jsonl"

        content = "\n".join(json.dumps(r, ensure_ascii=False) for r in buffer) + "\n"

        # dbutils.fs.put supports overwrite; to append, we create new file per batch
        dbutils.fs.put(log_file, content, overwrite=True)

    def log(self, level, message, run_id=None, object_name=None):
        record = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": level,
            "run_id": run_id,
            "object_name": object_name,
            "message": message,
        }
        self.log_queue.put(record)

    def info(self, msg, **kwargs): self.log("INFO", msg, **kwargs)
    def error(self, msg, **kwargs): self.log("ERROR", msg, **kwargs)
    def warning(self, msg, **kwargs): self.log("WARNING", msg, **kwargs)

    def stop(self):
        self.stop_event.set()
        self.thread.join(timeout=30)
