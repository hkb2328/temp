# databricks_rag_medallion_native.py
# End-to-end RAG system using Databricks-native embeddings and 3-layer Medallion architecture
# Bronze → Silver → Gold, all leveraging Databricks Vector Search & Model Serving

# -----------------------------
# Cell 0: Install dependencies (run once in notebook)
# -----------------------------
# %pip install databricks-vectorsearch

# -----------------------------
# Cell 1: Imports & Config
# -----------------------------
from pyspark.sql import SparkSession, functions as F, types as T
from databricks.vector_search.client import VectorSearchClient
from databricks.sdk import WorkspaceClient
from databricks.sdk.runtime import *
from PyPDF2 import PdfReader
import io

spark = SparkSession.builder.getOrCreate()
w = WorkspaceClient()

CATALOG = "your_catalog"
BRONZE_SCHEMA = "bronze"
SILVER_SCHEMA = "silver"
GOLD_SCHEMA = "gold"

BRONZE_TABLE = f"{CATALOG}.{BRONZE_SCHEMA}.pdf_bronze"
SILVER_TABLE = f"{CATALOG}.{SILVER_SCHEMA}.pdf_silver"
GOLD_TABLE = f"{CATALOG}.{GOLD_SCHEMA}.pdf_gold"

PDF_BASE_PATH = "/mnt/adls/business_pdfs/"

# Databricks-native models
EMBEDDING_MODEL = "databricks-bge-large-en"
LLM_MODEL = "databricks-dbrx-instruct"  # for answer generation
EMBEDDING_DIM = 1024
===========================================================================================================
def ingest_pdfs_to_bronze(pdf_base_path: str, bronze_table: str):
    # Read binary files directly from ADLS using Spark
    pdf_df = spark.read.format("binaryFile") \
        .option("pathGlobFilter", "*.pdf") \
        .option("recursiveFileLookup", "true") \
        .load(pdf_base_path)

    # Extract text from each PDF
    def extract_text(content):
        try:
            reader = PdfReader(io.BytesIO(content))
            return "\n".join([p.extract_text() or "" for p in reader.pages])
        except Exception:
            return ""

    extract_text_udf = F.udf(extract_text, T.StringType())

    pdf_df = pdf_df.withColumn("raw_text", extract_text_udf(F.col("content"))) \
                   .withColumn("ingest_ts", F.current_timestamp())

    pdf_df.select("path", "content", "raw_text", "ingest_ts") \
          .write.format("delta").mode("overwrite").option("overwriteSchema", "true") \
          .saveAsTable(bronze_table)

    print(f"✅ Bronze table {bronze_table} created successfully from {pdf_base_path}.")


VS_ENDPOINT_NAME = "pdf_rag_vs_endpoint"
VS_INDEX_NAME = f"{CATALOG}.{GOLD_SCHEMA}.pdf_gold_index"

# -----------------------------
# Cell 2: PDF Extraction Function
# -----------------------------
def extract_text_from_pdf_bytes(b: bytes) -> str:
    try:
        reader = PdfReader(io.BytesIO(b))
        text_pages = []
        for p in reader.pages:
            text_pages.append(p.extract_text() or "")
        return "\n".join(text_pages)
    except Exception:
        return ""

# -----------------------------
# Cell 3: Bronze - Raw Ingestion
# -----------------------------
def ingest_pdfs_to_bronze(pdf_base_path: str, bronze_table: str):
    bin_rdd = spark.sparkContext.binaryFiles(pdf_base_path)

    def read_and_extract(item):
        path, stream = item
        b = stream.read()
        text = extract_text_from_pdf_bytes(b)
        return (path, b, text)

    parsed = bin_rdd.map(read_and_extract)
    schema = T.StructType([
        T.StructField("path", T.StringType()),
        T.StructField("file_bytes", T.BinaryType()),
        T.StructField("raw_text", T.StringType()),
    ])
    df = spark.createDataFrame(parsed, schema)
    df = df.withColumn("ingest_ts", F.current_timestamp())

    df.write.format("delta").mode("overwrite").option("overwriteSchema", "true").saveAsTable(bronze_table)
    print(f"Bronze table {bronze_table} created.")

# ingest_pdfs_to_bronze(PDF_BASE_PATH, BRONZE_TABLE)

# -----------------------------
# Cell 4: Silver - Clean and Chunk Text
# -----------------------------
def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200):
    if not text:
        return []
    text = "\n".join([ln.strip() for ln in text.splitlines() if ln.strip()])
    chunks = []
    start = 0
    L = len(text)
    while start < L:
        end = min(start + chunk_size, L)
        chunks.append(text[start:end])
        start = end - overlap
        if start < 0:
            start = 0
    return chunks

chunk_udf = F.udf(lambda t: chunk_text(t, 1000, 200), T.ArrayType(T.StringType()))

def create_silver_from_bronze(bronze_table: str, silver_table: str):
    bronze = spark.table(bronze_table).select("path", "raw_text", "ingest_ts")
    silver = bronze.withColumn("chunks", chunk_udf(F.col("raw_text")))
    silver_exploded = silver.select(F.col("path"), F.posexplode(F.col("chunks")).alias("chunk_index", "chunk_text"), F.col("ingest_ts"))

    silver_final = silver_exploded.withColumn("source", F.col("path")) \
        .withColumn("source_id", F.sha2(F.col("path"), 256)) \
        .withColumn("doc_chunk_id", F.concat(F.col("source_id"), F.lit("_"), F.col("chunk_index"))) \
        .withColumn("chunk_len", F.length(F.col("chunk_text")))

    silver_final.write.format("delta").mode("overwrite").option("overwriteSchema", "true").saveAsTable(silver_table)
    print(f"Silver table {silver_table} created.")

# create_silver_from_bronze(BRONZE_TABLE, SILVER_TABLE)

# -----------------------------
# Cell 5: Gold - Databricks-native Embeddings
# -----------------------------
@F.udf(T.ArrayType(T.FloatType()))
def get_embedding_udf(text: str):
    if text is None or text.strip() == "":
        return []
    response = serving_endpoint_request(
        name=EMBEDDING_MODEL,
        data={"input": [text]},
    )
    return response["data"][0]["embedding"]

def create_gold_with_embeddings(silver_table: str, gold_table: str):
    silver_df = spark.table(silver_table)
    gold_df = silver_df.withColumn("embedding", get_embedding_udf(F.col("chunk_text")))
    gold_df.write.format("delta").mode("overwrite").option("overwriteSchema", "true").saveAsTable(gold_table)
    print(f"Gold table {gold_table} created with Databricks embeddings.")

# create_gold_with_embeddings(SILVER_TABLE, GOLD_TABLE)

# -----------------------------
# Cell 6: Vector Search Index (Delta Sync)
# -----------------------------
def create_vector_search_index(endpoint_name: str, index_name: str, gold_table: str, embedding_col: str, embedding_dim: int):
    client = VectorSearchClient()

    try:
        client.get_endpoint(endpoint_name)
    except:
        print(f"Creating new endpoint {endpoint_name}")
        client.create_endpoint(endpoint_name)

    client.create_delta_sync_index(
        endpoint_name=endpoint_name,
        source_table_name=gold_table,
        index_name=index_name,
        primary_key="doc_chunk_id",
        embedding_vector_column=embedding_col,
        embedding_dimension=embedding_dim,
        pipeline_type="TRIGGERED"
    )

    print(f"Vector Search index {index_name} created.")

# create_vector_search_index(VS_ENDPOINT_NAME, VS_INDEX_NAME, GOLD_TABLE, 'embedding', EMBEDDING_DIM)

# -----------------------------
# Cell 7: Query Vector Index
# -----------------------------
def query_vector_index(index_object, query_text: str, top_k: int = 5):
    q_response = serving_endpoint_request(
        name=EMBEDDING_MODEL,
        data={"input": [query_text]},
    )
    q_vector = q_response["data"][0]["embedding"]
    results = index_object.similarity_search(query_vector=q_vector, num_results=top_k)
    out = []
    for r in results:
        out.append({
            'doc_chunk_id': r.get('id'),
            'distance': r.get('distance'),
            'metadata': {k: v for k, v in r.items() if k not in ('id','vector')}
        })
    return out

# -----------------------------
# Cell 8: RAG Answer Generation (with databricks-dbrx-instruct)
# -----------------------------
def generate_answer_from_context(query_text: str, retrieved_docs: list):
    context = "\n\n".join([d['metadata'].get('chunk_text', '') for d in retrieved_docs])
    prompt = f"""
You are a helpful assistant. Use the following document excerpts to answer the user's question.

CONTEXT:
{context}

QUESTION:
{query_text}

Please give a concise, factual answer based only on the provided context.
"""
    llm_response = serving_endpoint_request(
        name=LLM_MODEL,
        data={"input": prompt}
    )
    answer = llm_response["predictions"][0]["text"] if "predictions" in llm_response else llm_response
    return answer

# Example full pipeline usage:
# client = VectorSearchClient()
# index_obj = client.get_index(VS_ENDPOINT_NAME, VS_INDEX_NAME)
# query = "What is the cancellation policy for premium customers?"
# retrieved = query_vector_index(index_obj, query, top_k=5)
# answer = generate_answer_from_context(query, retrieved)
# print(answer)

# -----------------------------
# Notes
# -----------------------------
# 1. Fully Databricks-native RAG pipeline (no external dependencies).
# 2. Uses databricks-bge-large-en for embeddings, databricks-dbrx-instruct for generation.
# 3. Works with Delta + Vector Search for scalable retrieval.
# 4. Ideal for enterprise knowledge base Q&A over PDFs.
====================================================================================================================================
def ingest_pdfs_to_bronze(pdf_base_path: str, bronze_table: str):
    # Read binary files directly from ADLS using Spark
    pdf_df = spark.read.format("binaryFile") \
        .option("pathGlobFilter", "*.pdf") \
        .option("recursiveFileLookup", "true") \
        .load(pdf_base_path)

    # Extract text from each PDF
    def extract_text(content):
        try:
            reader = PdfReader(io.BytesIO(content))
            return "\n".join([p.extract_text() or "" for p in reader.pages])
        except Exception:
            return ""

    extract_text_udf = F.udf(extract_text, T.StringType())

    pdf_df = pdf_df.withColumn("raw_text", extract_text_udf(F.col("content"))) \
                   .withColumn("ingest_ts", F.current_timestamp())

    pdf_df.select("path", "content", "raw_text", "ingest_ts") \
          .write.format("delta").mode("overwrite").option("overwriteSchema", "true") \
          .saveAsTable(bronze_table)

    print(f"✅ Bronze table {bronze_table} created successfully from {pdf_base_path}.")

=======================================================================================================================================================================
from pyspark.sql import functions as F, types as T

def create_silver_from_bronze_single_pdf(bronze_table: str, silver_table: str):
    # Read the one PDF's text directly
    bronze_df = spark.table(bronze_table).select("path", "raw_text", "ingest_ts").limit(1)
    pdf_row = bronze_df.collect()[0]
    path = pdf_row["path"]
    text = pdf_row["raw_text"]
    ingest_ts = pdf_row["ingest_ts"]

    # Simple chunking in Python
    def chunk_text(text, chunk_size=1000, overlap=200):
        if not text:
            return []
        text = "\n".join([ln.strip() for ln in text.splitlines() if ln.strip()])
        chunks = []
        start = 0
        while start < len(text):
            end = min(start + chunk_size, len(text))
            chunks.append(text[start:end])
            start = end - overlap
            if start < 0:
                start = 0
        return chunks

    chunks = chunk_text(text)

    # Convert back to Spark DataFrame
    rows = [(path, i, c, ingest_ts) for i, c in enumerate(chunks)]
    schema = T.StructType([
        T.StructField("path", T.StringType()),
        T.StructField("chunk_index", T.IntegerType()),
        T.StructField("chunk_text", T.StringType()),
        T.StructField("ingest_ts", T.TimestampType())
    ])
    silver_df = spark.createDataFrame(rows, schema)

    # Add metadata
    silver_final = silver_df.withColumn("source_id", F.sha2(F.col("path"), 256)) \
        .withColumn("doc_chunk_id", F.concat(F.col("source_id"), F.lit("_"), F.col("chunk_index"))) \
        .withColumn("chunk_len", F.length(F.col("chunk_text")))

    silver_final.write.format("delta").mode("overwrite").option("overwriteSchema", "true").saveAsTable(silver_table)
    print(f"✅ Silver table {silver_table} created successfully (single PDF mode).")
