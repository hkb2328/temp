"""
Databricks RAG Pipeline using built-in foundation_model API
------------------------------------------------------------
‚úÖ Works with Databricks Runtime 16.4 LTS ML+
‚úÖ Uses official Databricks embedding & LLM models
‚úÖ No external API calls or tokens
‚úÖ Reads PDFs from ADLS and stores embeddings in a Delta table
"""

import uuid
import numpy as np
from io import BytesIO
from typing import List
from pyspark.sql import Row
from pyspark.sql.functions import col
from pyspark.sql.types import ArrayType, FloatType

# ---------- CONFIG ----------
PDF_FOLDER_PATH = "dbfs:/mnt/adls/path/to/pdfs/"  # your ADLS folder
EMBEDDED_TABLE = "your_catalog.your_schema.silver_embeddings"  # update this
MAX_WORDS = 300
OVERLAP = 50
TOP_K = 5
# ----------------------------

# ‚úÖ Import new Databricks GenAI interface
from databricks.model_training import foundation_model as fm

# Load embedding & chat models
emb_model = fm.create(name="databricks-bge-large-en")
llm = fm.create(name="databricks-dbrx-instruct")

# ---------- PDF Utilities ----------
try:
    from PyPDF2 import PdfReader
except ImportError:
    import sys, subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "PyPDF2"])
    from PyPDF2 import PdfReader

def extract_text_from_pdf_bytes(pdf_bytes: bytes) -> str:
    """Extract text from PDF bytes"""
    reader = PdfReader(BytesIO(pdf_bytes))
    return "\n".join(page.extract_text() or "" for page in reader.pages)

def chunk_text(text: str, max_words=MAX_WORDS, overlap=OVERLAP) -> List[str]:
    """Split long text into overlapping chunks"""
    words = text.split()
    chunks = []
    start = 0
    while start < len(words):
        end = start + max_words
        chunk = " ".join(words[start:end])
        chunks.append(chunk)
        start = end - overlap
        if start < 0:
            start = 0
    return [c for c in chunks if c.strip()]

# ---------- ADLS Utilities ----------
def list_pdfs(folder_path: str):
    """List all PDFs from ADLS folder"""
    return [f.path for f in dbutils.fs.ls(folder_path) if f.name.lower().endswith(".pdf")]

def read_pdf_bytes(path: str) -> bytes:
    """Read binary content of a PDF file"""
    df = spark.read.format("binaryFile").load(path)
    return bytes(df.collect()[0]["content"])

# ---------- Embedding Creation ----------
def process_and_store_embeddings(pdf_folder: str, embedded_table: str):
    """Read PDFs, create chunks & embeddings, write to Delta table"""
    pdf_paths = list_pdfs(pdf_folder)
    all_rows = []

    for path in pdf_paths:
        print(f"üìÑ Processing: {path}")
        raw = read_pdf_bytes(path)
        text = extract_text_from_pdf_bytes(raw)
        chunks = chunk_text(text)

        # Generate embeddings for each chunk
        embeddings = emb_model.predict(chunks)
        doc_id = path.replace("/", "_")

        for chunk, emb in zip(chunks, embeddings):
            all_rows.append(Row(
                chunk_id=str(uuid.uuid4()),
                doc_id=doc_id,
                chunk_text=chunk,
                embedding=emb
            ))

    if not all_rows:
        print("‚ö†Ô∏è No PDF text chunks found.")
        return

    df = spark.createDataFrame(all_rows)
    df = df.withColumn("embedding", col("embedding").cast(ArrayType(FloatType())))
    df.write.format("delta").mode("overwrite").saveAsTable(embedded_table)
    print(f"‚úÖ Embeddings table written to {embedded_table}")

# ---------- Retrieval + Generation ----------
def cosine_similarity(a: np.ndarray, b: np.ndarray):
    denom = (np.linalg.norm(a) * np.linalg.norm(b))
    return float(np.dot(a, b) / denom) if denom != 0 else 0.0

def retrieve_top_k(query: str, embedded_table: str, top_k=TOP_K):
    """Retrieve top-k most relevant chunks for a query"""
    query_emb = np.array(emb_model.predict([query])[0])
    df = spark.table(embedded_table).collect()

    scored = []
    for r in df:
        emb = np.array(r["embedding"])
        score = cosine_similarity(query_emb, emb)
        scored.append((r["chunk_id"], r["doc_id"], r["chunk_text"], score))

    top = sorted(scored, key=lambda x: x[3], reverse=True)[:top_k]
    return top

def retrieve_and_generate(query: str, embedded_table: str, top_k=TOP_K):
    """Perform retrieval and generate LLM answer"""
    top_chunks = retrieve_top_k(query, embedded_table, top_k)
    context = "\n\n".join([r[2] for r in top_chunks])

    prompt = f"""
    You are a helpful assistant. Use the context below to answer the question.
    If the answer is not in the context, say "I don't know."

    Context:
    {context}

    Question: {query}
    Answer:
    """

    # Generate response
    result = llm.predict(prompt)
    return result

# ---------- Main ----------
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Databricks RAG with foundation_model API")
    parser.add_argument("--create_embeddings", action="store_true", help="Process PDFs and create embeddings")
    parser.add_argument("--query", type=str, default=None, help="Ask a question to the RAG model")
    args = parser.parse_args()

    if args.create_embeddings:
        process_and_store_embeddings(PDF_FOLDER_PATH, EMBEDDED_TABLE)

    if args.query:
        print("üîç Running RAG query...")
        ans = retrieve_and_generate(args.query, EMBEDDED_TABLE)
        print("\n=== ANSWER ===\n")
        print(ans)
