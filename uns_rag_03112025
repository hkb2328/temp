"""
Databricks RAG Pipeline (No Vector Index, using built-in GenAI models)
---------------------------------------------------------------------
‚úÖ Works entirely inside Databricks ML cluster
‚úÖ Uses built-in Databricks models (no REST calls)
‚úÖ Writes to Unity Catalog Delta table
"""

import uuid
import numpy as np
from io import BytesIO
from typing import List
from pyspark.sql import Row
from pyspark.sql.functions import col, udf
from pyspark.sql.types import ArrayType, FloatType

# ---------- CONFIG ----------
PDF_FOLDER_PATH = "dbfs:/mnt/adls/path/to/pdfs/"  # your ADLS path
EMBEDDED_TABLE = "your_catalog.your_schema.silver_embeddings"  # change this
MAX_WORDS = 300
OVERLAP = 50
TOP_K = 5
# ----------------------------

# Import Databricks built-in GenAI library
from databricks import genai

# Load embedding & generation models
emb_model = genai.embeddings.get("databricks-bge-large-en")
llm = genai.ChatModel("databricks-dbrx-instruct")

# ---------- PDF Utilities ----------
try:
    from PyPDF2 import PdfReader
except ImportError:
    import sys, subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "PyPDF2"])
    from PyPDF2 import PdfReader

def extract_text_from_pdf_bytes(pdf_bytes: bytes) -> str:
    reader = PdfReader(BytesIO(pdf_bytes))
    text = []
    for page in reader.pages:
        text.append(page.extract_text() or "")
    return "\n".join(text)

def chunk_text(text: str, max_words=MAX_WORDS, overlap=OVERLAP) -> List[str]:
    words = text.split()
    chunks = []
    start = 0
    while start < len(words):
        end = start + max_words
        chunk = " ".join(words[start:end])
        chunks.append(chunk)
        start = end - overlap
        if start < 0:
            start = 0
    return [c for c in chunks if c.strip()]

# ---------- ADLS Utilities ----------
def list_pdfs(folder_path: str):
    return [f.path for f in dbutils.fs.ls(folder_path) if f.name.lower().endswith(".pdf")]

def read_pdf_bytes(path: str) -> bytes:
    df = spark.read.format("binaryFile").load(path)
    return bytes(df.collect()[0]["content"])

# ---------- Embedding Creation ----------
def process_and_store_embeddings(pdf_folder: str, embedded_table: str):
    pdf_paths = list_pdfs(pdf_folder)
    all_rows = []

    for path in pdf_paths:
        print(f"Processing {path}")
        raw = read_pdf_bytes(path)
        text = extract_text_from_pdf_bytes(raw)
        chunks = chunk_text(text)

        embeddings = emb_model.embed_text(chunks)
        doc_id = path.replace("/", "_")

        for chunk, emb in zip(chunks, embeddings):
            all_rows.append(Row(
                chunk_id=str(uuid.uuid4()),
                doc_id=doc_id,
                chunk_text=chunk,
                embedding=emb
            ))

    if not all_rows:
        print("‚ö†Ô∏è No chunks found.")
        return

    df = spark.createDataFrame(all_rows)
    df = df.withColumn("embedding", col("embedding").cast(ArrayType(FloatType())))
    df.write.format("delta").mode("overwrite").saveAsTable(embedded_table)
    print(f"‚úÖ Embeddings table written to {embedded_table}")

# ---------- Retrieval + Generation ----------
def cosine_similarity(a: np.ndarray, b: np.ndarray):
    denom = (np.linalg.norm(a) * np.linalg.norm(b))
    if denom == 0:
        return 0.0
    return float(np.dot(a, b) / denom)

def retrieve_top_k(query: str, embedded_table: str, top_k=TOP_K):
    query_emb = np.array(emb_model.embed_text([query])[0])
    df = spark.table(embedded_table).collect()

    scored = []
    for r in df:
        emb = np.array(r["embedding"])
        score = cosine_similarity(query_emb, emb)
        scored.append((r["chunk_id"], r["doc_id"], r["chunk_text"], score))

    top = sorted(scored, key=lambda x: x[3], reverse=True)[:top_k]
    return top

def retrieve_and_generate(query: str, embedded_table: str, top_k=TOP_K):
    top_chunks = retrieve_top_k(query, embedded_table, top_k)
    context = "\n\n".join([f"{r[2]}" for r in top_chunks])

    prompt = f"""
    You are a helpful assistant. Use the context below to answer the question.
    If the answer is not in the context, say "I don't know".

    Context:
    {context}

    Question: {query}
    Answer:
    """

    response = llm.chat(prompt)
    return response

# ---------- Main ----------
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Databricks RAG with built-in GenAI")
    parser.add_argument("--create_embeddings", action="store_true")
    parser.add_argument("--query", type=str, default=None)
    args = parser.parse_args()

    if args.create_embeddings:
        process_and_store_embeddings(PDF_FOLDER_PATH, EMBEDDED_TABLE)

    if args.query:
        print("üîç Running RAG query...")
        ans = retrieve_and_generate(args.query, EMBEDDED_TABLE)
        print("\n=== ANSWER ===\n")
        print(ans)
