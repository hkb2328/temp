"""
Databricks RAG Pipeline using built-in foundation_model API
------------------------------------------------------------
‚úÖ Works with Databricks Runtime 16.4 LTS ML+
‚úÖ Uses official Databricks embedding & LLM models
‚úÖ No external API calls or tokens
‚úÖ Reads PDFs from ADLS and stores embeddings in a Delta table
"""

import uuid
import numpy as np
from io import BytesIO
from typing import List
from pyspark.sql import Row
from pyspark.sql.functions import col
from pyspark.sql.types import ArrayType, FloatType

# ---------- CONFIG ----------
PDF_FOLDER_PATH = "dbfs:/mnt/adls/path/to/pdfs/"  # your ADLS folder
EMBEDDED_TABLE = "your_catalog.your_schema.silver_embeddings"  # update this
MAX_WORDS = 300
OVERLAP = 50
TOP_K = 5
# ----------------------------

# ‚úÖ Import new Databricks GenAI interface
from databricks.model_training import foundation_model as fm

# Load embedding & chat models
emb_model = fm.create(name="databricks-bge-large-en")
llm = fm.create(name="databricks-dbrx-instruct")

# ---------- PDF Utilities ----------
try:
    from PyPDF2 import PdfReader
except ImportError:
    import sys, subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "PyPDF2"])


    from PyPDF2 import PdfReader

def extract_text_from_pdf_bytes(pdf_bytes: bytes) -> str:
    """Extract text from PDF bytes"""
    reader = PdfReader(BytesIO(pdf_bytes))
    return "\n".join(page.extract_text() or "" for page in reader.pages)

def chunk_text(text: str, max_words=MAX_WORDS, overlap=OVERLAP) -> List[str]:
    """Split long text into overlapping chunks"""
    words = text.split()
    chunks = []
    start = 0
    while start < len(words):
        end = start + max_words
        chunk = " ".join(words[start:end])
        chunks.append(chunk)
        start = end - overlap
        if start < 0:
            start = 0
    return [c for c in chunks if c.strip()]

# ---------- ADLS Utilities ----------
def list_pdfs(folder_path: str):
    """List all PDFs from ADLS folder"""
    return [f.path for f in dbutils.fs.ls(folder_path) if f.name.lower().endswith(".pdf")]

def read_pdf_bytes(path: str) -> bytes:
    """Read binary content of a PDF file"""
    df = spark.read.format("binaryFile").load(path)
    return bytes(df.collect()[0]["content"])

# ---------- Embedding Creation ----------
def process_and_store_embeddings(pdf_folder: str, embedded_table: str):
    """Read PDFs, create chunks & embeddings, write to Delta table"""
    pdf_paths = list_pdfs(pdf_folder)
    all_rows = []

    for path in pdf_paths:
        print(f"üìÑ Processing: {path}")
        raw = read_pdf_bytes(path)
        text = extract_text_from_pdf_bytes(raw)
        chunks = chunk_text(text)

        # Generate embeddings for each chunk
        embeddings = emb_model.predict(chunks)
        doc_id = path.replace("/", "_")

        for chunk, emb in zip(chunks, embeddings):
            all_rows.append(Row(
                chunk_id=str(uuid.uuid4()),
                doc_id=doc_id,
                chunk_text=chunk,
                embedding=emb
            ))

    if not all_rows:
        print("‚ö†Ô∏è No PDF text chunks found.")
        return

    df = spark.createDataFrame(all_rows)
    df = df.withColumn("embedding", col("embedding").cast(ArrayType(FloatType())))
    df.write.format("delta").mode("overwrite").saveAsTable(embedded_table)
    print(f"‚úÖ Embeddings table written to {embedded_table}")

# ---------- Retrieval + Generation ----------
def cosine_similarity(a: np.ndarray, b: np.ndarray):
    denom = (np.linalg.norm(a) * np.linalg.norm(b))
    return float(np.dot(a, b) / denom) if denom != 0 else 0.0

def retrieve_top_k(query: str, embedded_table: str, top_k=TOP_K):
    """Retrieve top-k most relevant chunks for a query"""
    query_emb = np.array(emb_model.predict([query])[0])
    df = spark.table(embedded_table).collect()

    scored = []
    for r in df:
        emb = np.array(r["embedding"])
        score = cosine_similarity(query_emb, emb)
        scored.append((r["chunk_id"], r["doc_id"], r["chunk_text"], score))

    top = sorted(scored, key=lambda x: x[3], reverse=True)[:top_k]
    return top

def retrieve_and_generate(query: str, embedded_table: str, top_k=TOP_K):
    """Perform retrieval and generate LLM answer"""
    top_chunks = retrieve_top_k(query, embedded_table, top_k)
    context = "\n\n".join([r[2] for r in top_chunks])

    prompt = f"""
    You are a helpful assistant. Use the context below to answer the question.
    If the answer is not in the context, say "I don't know."

    Context:
    {context}

    Question: {query}
    Answer:
    """

    # Generate response
    result = llm.predict(prompt)
    return result

# ---------- Main ----------
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Databricks RAG with foundation_model API")
    parser.add_argument("--create_embeddings", action="store_true", help="Process PDFs and create embeddings")
    parser.add_argument("--query", type=str, default=None, help="Ask a question to the RAG model")
    args = parser.parse_args()

    if args.create_embeddings:
        process_and_store_embeddings(PDF_FOLDER_PATH, EMBEDDED_TABLE)

    if args.query:
        print("üîç Running RAG query...")
        ans = retrieve_and_generate(args.query, EMBEDDED_TABLE)
        print("\n=== ANSWER ===\n")
        print(ans)
=================================================================================================================================================
from pyspark.sql import SparkSession
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.llms import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# Initialize Spark
spark = SparkSession.builder.appName("RAG_PDF_LOCAL").getOrCreate()

# ===== 1Ô∏è‚É£ Load PDF from ADLS =====
# Make sure your ADLS path is accessible via mount or abfss://
pdf_path = "abfss://<container>@<storage-account>.dfs.core.windows.net/<path>/your_doc.pdf"
loader = PyPDFLoader(pdf_path)
docs = loader.load()

# ===== 2Ô∏è‚É£ Split into chunks =====
splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200)
chunks = splitter.split_documents(docs)

# ===== 3Ô∏è‚É£ Create local embeddings =====
# Uses a lightweight model that runs locally, no API needed
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# ===== 4Ô∏è‚É£ Store embeddings in FAISS =====
vectorstore = FAISS.from_documents(chunks, embeddings)

# ===== 5Ô∏è‚É£ Load a small local LLM =====
# This uses the free DistilGPT-2 model from Hugging Face Hub (auto-downloaded)
model_id = "distilgpt2"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=200,
    temperature=0.3
)

llm = HuggingFacePipeline(pipeline=pipe)

# ===== 6Ô∏è‚É£ Build a RetrievalQA chain =====
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
    return_source_documents=False
)

# ===== 7Ô∏è‚É£ Ask your question =====
query = "Summarize the main points of this document."
result = qa_chain({"query": query})

print("\n=== Answer ===\n")
print(result["result"])
