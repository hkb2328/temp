from datetime import datetime

class CustomLogger:
    """
    Databricks-compatible custom logger that writes log messages directly to ADLS.
    Each log line is appended as text into a .log file stored in your ADLS container.
    """

    def __init__(self, spark, adls_base_path):
        """
        :param spark: Active SparkSession object
        :param adls_base_path: Base ADLS path, e.g.
               'abfss://raw@youraccount.dfs.core.windows.net/logs/fusion_pipeline_logs'
        """
        self.spark = spark
        self.adls_base_path = adls_base_path

    def _write_log(self, log_line, run_id):
        """Internal helper to append a log line into ADLS."""
        log_df = self.spark.createDataFrame([(log_line,)], ["log"])
        adls_path = f"{self.adls_base_path}/run_id={run_id}/fusion_log_{datetime.now().strftime('%Y%m%d')}.log"
        log_df.write.mode("append").text(adls_path)

    def log(self, run_id, level, message, object_name=None, attempt=None):
        """
        Write a log entry to ADLS.
        :param run_id: Unique pipeline run ID
        :param level: INFO / ERROR / WARNING / DEBUG
        :param message: Log message
        :param object_name: Optional (for object-level logging)
        :param attempt: Optional (retry attempt)
        """
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_line = f"{timestamp} | {level.upper()} | run_id={run_id} | object={object_name or '-'} | attempt={attempt or '-'} | {message}"
        self._write_log(log_line, run_id)


=================================================================================================================================================

import os
import time
import uuid
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

from GR_DATAHUB.src.common.utils import commonUtils
from GR_DATAHUB.src.fusion.medallion_pipeline import FusionLakeHousePipeline
from custom_logger import CustomLogger


class Fusion_runtime:
    def __init__(self, spark):
        self.spark = spark
        # ðŸ‘‡ Update your ADLS container & storage account here
        self.logger = CustomLogger(
            spark,
            "abfss://raw@<your-storage-account>.dfs.core.windows.net/logs/fusion_pipeline_logs",
        )

    def generate_run_id(self):
        """Generate a unique run ID for each pipeline run."""
        uid = uuid.uuid4()
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        return f"{timestamp}_{str(uid)[:8]}"

    def process_object(self, each, run_id, env, max_retries=3, retry_wait=10):
        """
        Processes each fusion object with retries and logging.
        """
        object_name = each["object_name"]
        query = each["query"]
        schema_path = each["schema_path"]
        select_columns = each["select_columns"]
        primary_key = each["primary_key"]
        sensitive_columns = each.get("sensitive_columns", {})

        self.logger.log(run_id, "INFO", "Starting ingestion", object_name)
        attempt = 0

        while attempt < max_retries:
            try:
                # Load schema and trigger ingestion
                schema = commonUtils(self.spark).load_schema(schema_path)
                FusionLakeHousePipeline(self.spark, env).raw_ingest(
                    object_name, query, schema, primary_key, select_columns, run_id, sensitive_columns
                )

                # Log success
                self.logger.log(run_id, "INFO", f"Successfully ingested {object_name} on attempt {attempt + 1}", object_name, attempt + 1)
                self.spark.catalog.clearCache()
                break

            except Exception as e:
                attempt += 1
                err_msg = f"{object_name} failed on attempt {attempt} with error: {e}"
                self.logger.log(run_id, "ERROR", err_msg, object_name, attempt)

                if attempt >= max_retries:
                    self.logger.log(run_id, "ERROR", f"{object_name} failed after {max_retries} retries.", object_name)
                else:
                    time.sleep(retry_wait)


# ============================================================
#                   MAIN EXECUTION LOGIC
# ============================================================

if __name__ == "__main__":
    try:
        env = os.getenv("ENV", "DEV")
        fusion_runtime = Fusion_runtime(spark)
        run_id = fusion_runtime.generate_run_id()
        fusion_objects_config = commonUtils(spark).load_yaml("fusion_config.yaml")

        fusion_runtime.logger.log(run_id, "INFO", "Pipeline execution started")

        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = [
                executor.submit(fusion_runtime.process_object, each, run_id, env)
                for each in fusion_objects_config["fusion_objects"]
            ]

            for future in as_completed(futures):
                try:
                    future.result()
                    spark.catalog.clearCache()
                except Exception as exc:
                    fusion_runtime.logger.log(run_id, "ERROR", f"Thread execution failed: {exc}")

        fusion_runtime.logger.log(run_id, "INFO", "Pipeline execution completed successfully")

    except Exception as e:
        fusion_runtime.logger.log(run_id, "CRITICAL", f"Pipeline crashed: {e}")
        raise
